{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "#Loading nltk library and its packages for text processing.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Downloading some necessary word lists.\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#Downloading all dependencies and corpora.\n",
    "#nltk.download()\n",
    "#nltk.download('all', halt_on_error=False)\n",
    "\n",
    "#Importing pattern and its dependencies.\n",
    "import pattern\n",
    "from pattern.en import tag\n",
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-4b5d7af4e521>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-4b5d7af4e521>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install setup.py\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Need to install this everytime, before using pattern!\n",
    "pip install setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Training and Testing Datasets.\n",
    "train_data = pd.read_csv(\"C:/Users/Bhair/OneDrive - University of Oklahoma/Master_of_Science_Data/Data Science Practice/Twitter Sentiment Analysis/Data/tweet-sentiment-extraction/train.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Bhair/OneDrive - University of Oklahoma/Master_of_Science_Data/Data Science Practice/Twitter Sentiment Analysis/Data/tweet-sentiment-extraction/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#Contraction directory.\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Function for expanding contractions using above dictonary of contractions.\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    #Creating a list of contraction keys.\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags = re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    #Function for expanding the contractions.\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = (contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower()))\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "\n",
    "\n",
    "#Function for creating a list ofstrings from the input. It creates different elements in the list if there is a space.\n",
    "#Basically, a sentence will be converted into a list of words.\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "#Reading the file with vocabulary.\n",
    "WORDS = Counter(words(open('C:/Users/Bhair/OneDrive - University of Oklahoma/Master_of_Science_Data/Data Science Practice/Twitter Sentiment Analysis/big.txt').read()))\n",
    "\n",
    "#Function for calculating probability of the given word to find in the vocabulary.\n",
    "def P(word, N = sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    #WORDS[word] gives the count of word\n",
    "    #N is total number of words in the vocabulary.\n",
    "    return WORDS[word] / N\n",
    "\n",
    "#Main function which should be called for word correction.\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key = P)\n",
    "\n",
    "#Function for genetraing all possible real words from given string.\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "#Function for editing given string. This returns words which are one edit away from input string.\n",
    "#The function edits1 returns a set of all the edited strings (whether words or not) that can be made with one simple edit: \n",
    "#a simple edit to a word is a deletion (remove one letter), a transposition (swap two adjacent letters), a replacement (change one letter to another) or an insertion (add a letter).\n",
    "#The output of this function can be a big set. For a word of length n, there will be n deletions, n-1 transpositions, \n",
    "#26n alterations, and 26(n+1) insertions, for a total of 54n+25 (of which a few are typically duplicates)\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    #Splitting the letters of a word to form a combination.\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "#Function for generating set of words which are two edits away from input string.\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "#Function for converting a list to a string.\n",
    "def listToString(inlist):\n",
    "    #Initialize an empty string.\n",
    "    string = \" \"\n",
    "    return (string.join(inlist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function for preprocessing and normalizing text data.\n",
    "def data_normalization(train_data):\n",
    "    #Setting stop word variable using NLTK package.\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    #Creating list of punctuations.\n",
    "    punctuations = [\".\", \",\", \"!\", \"`\", \"...\", \"****\", \":\", \"(\", \")\", \"?\", \"-\", \"{\", \"}\", \"[\", \"]\", \";\", \"/\", \"+\", \"<\", \n",
    "               \">\", \"|\", \"=\" \"_\", \"^\", \"@\", \"#\", \"$\", \"%\", \"~\", \"'\", \"..\"]\n",
    "    \n",
    "    #Regular expression tokenizer with only alphanumeric contents.\n",
    "    Token_Pattern = r'\\w+'\n",
    "    regex_wt = nltk.RegexpTokenizer(pattern = Token_Pattern, gaps = False)\n",
    "\n",
    "    #Listing out all contractions.\n",
    "    contaction_keys = list(CONTRACTION_MAP.keys())\n",
    "\n",
    "    #Declaring list for storing lemmatized tweets.\n",
    "    lemmatized_words = []\n",
    "    for i in range(len(train_data[\"text\"])):\n",
    "        #Condition for empty tweets.\n",
    "        if (str(train_data[\"text\"][i]) == \"nan\"):\n",
    "            #Fake strink in the blank tweet.\n",
    "            train_data[\"text\"][i] = \"NA\"\n",
    "        #Expanding contractions.\n",
    "        #print(\"Before correction: \", train_data[\"text\"][i])\n",
    "        text_segment = train_data[\"text\"][i].replace(\"`\", \"'\")\n",
    "        expanded_tweets = expand_contractions(text_segment, CONTRACTION_MAP)\n",
    "        #Removing URL links from tweets.\n",
    "        expanded_tweets = re.sub(r'http\\S+', '', expanded_tweets, flags = re.MULTILINE)\n",
    "        #print(\"After contractions: \", expanded_tweets)\n",
    "        #Tokenizing the words.\n",
    "        tokens = regex_wt.tokenize(expanded_tweets)\n",
    "        #Declaring list for lemmatized words.\n",
    "        lemm_list = []\n",
    "        for word in tokens:\n",
    "            #Condition for stopwords.\n",
    "            if (word not in stopWords):\n",
    "                #Condition for punctuations.\n",
    "                if (word not in punctuations):\n",
    "                    #Correcting the word.\n",
    "                    #Not using this function because its not correcting words properly. Need to find more accurate way!\n",
    "                    #corrected_spell = correction(word)\n",
    "                    corrected_spell = word\n",
    "                    #Converting all letters to lower form.\n",
    "                    corrected_spell = corrected_spell.lower()\n",
    "                    #Lemmatizing words.\n",
    "                    lemm_list.append(wordnet_lemmatizer.lemmatize(corrected_spell, pos = \"v\"))\n",
    "        #Converting the lemmatized word's (document) list to a string.\n",
    "        lemm_string = listToString(lemm_list)\n",
    "        lemmatized_words.append(lemm_string)\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i would respond i go',\n",
       " 'sooo sad i miss san diego',\n",
       " 'boss bully',\n",
       " 'interview leave alone',\n",
       " 'sons could put release already buy',\n",
       " 'shameless plug best rangers forum earth',\n",
       " '2am feed baby fun smile coo',\n",
       " 'soooo high',\n",
       " 'both',\n",
       " 'journey wow u become cooler hehe possible']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = data_normalization(train_data[:10])\n",
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Last session of the day  http://twitpic.com/67ezh\n",
       "1     Shanghai is also really exciting (precisely -...\n",
       "2    Recession hit Veronique Branquinho, she has to...\n",
       "3                                          happy bday!\n",
       "4               http://twitpic.com/4w75p - I like it!!\n",
       "5                      that`s great!! weee!! visitors!\n",
       "6              I THINK EVERYONE HATES ME ON HERE   lol\n",
       "7     soooooo wish i could, but im in school and my...\n",
       "8     and within a short time of the last clue all ...\n",
       "9     What did you get?  My day is alright.. haven`...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last session day',\n",
       " 'shanghai also really excite precisely skyscrapers galore good tweeps china sh bj',\n",
       " 'recession hit veronique branquinho quit company shame',\n",
       " 'happy bday',\n",
       " 'i like',\n",
       " 'great weee visitors',\n",
       " 'i think everyone hat me on here lol',\n",
       " 'soooooo wish could im school myspace completely block',\n",
       " 'within short time last clue',\n",
       " 'what get my day alright do anything yet leave soon stepsister though']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = data_normalization(test_data[:10])\n",
    "new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORPUS = ['the sky is blue', 'sky is blue and sky is beautiful', 'the beautiful sky is so blue', 'i love blue cheese']\n",
    "#new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Function for vectorizing a given document using \"Bag of words\".\n",
    "def bow_extractor(corpus, ngram_range = (1, 1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0\n",
      "  1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0\n",
      "  0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "['2am', 'alone', 'already', 'baby', 'become', 'best', 'boss', 'both', 'bully', 'buy', 'coo', 'cooler', 'could', 'diego', 'earth', 'feed', 'forum', 'fun', 'go', 'hehe', 'high', 'interview', 'journey', 'leave', 'miss', 'plug', 'possible', 'put', 'rangers', 'release', 'respond', 'sad', 'san', 'shameless', 'smile', 'sons', 'sooo', 'soooo', 'would', 'wow']\n"
     ]
    }
   ],
   "source": [
    "#Building bow vectorizer and extracting features.\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "\n",
    "#Densing features.\n",
    "features = bow_features.todense()\n",
    "print(features)\n",
    "\n",
    "#Extracting features from test dataset using above built vectorizer.\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "#Densing features.\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(new_doc_features)\n",
    "\n",
    "#Print the feature names.\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for displaying features.\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data = features, columns = feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2am  alone  already  baby  become  best  boss  both  bully   buy  ...  \\\n",
      "0  0.00   0.00     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "1  0.00   0.00     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "2  0.00   0.00     0.00  0.00    0.00  0.00  0.71   0.0   0.71  0.00  ...   \n",
      "3  0.00   0.58     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "4  0.00   0.00     0.41  0.00    0.00  0.00  0.00   0.0   0.00  0.41  ...   \n",
      "5  0.00   0.00     0.00  0.00    0.00  0.41  0.00   0.0   0.00  0.00  ...   \n",
      "6  0.41   0.00     0.00  0.41    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "7  0.00   0.00     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "8  0.00   0.00     0.00  0.00    0.00  0.00  0.00   1.0   0.00  0.00  ...   \n",
      "9  0.00   0.00     0.00  0.00    0.41  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "\n",
      "   respond   sad   san  shameless  smile  sons  sooo  soooo  would   wow  \n",
      "0     0.58  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.58  0.00  \n",
      "1     0.00  0.45  0.45       0.00   0.00  0.00  0.45   0.00   0.00  0.00  \n",
      "2     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "3     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "4     0.00  0.00  0.00       0.00   0.00  0.41  0.00   0.00   0.00  0.00  \n",
      "5     0.00  0.00  0.00       0.41   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "6     0.00  0.00  0.00       0.00   0.41  0.00  0.00   0.00   0.00  0.00  \n",
      "7     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.71   0.00  0.00  \n",
      "8     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "9     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.41  \n",
      "\n",
      "[10 rows x 40 columns]\n",
      "   2am  alone  already  baby  become  best  boss  both  bully  buy  ...  \\\n",
      "0  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "1  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "2  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "3  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "4  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "5  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "6  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "7  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "8  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "9  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "\n",
      "   respond  sad  san  shameless  smile  sons  sooo  soooo  would  wow  \n",
      "0      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "1      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "2      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "3      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "4      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "5      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "6      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "7      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "8      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "9      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "\n",
      "[10 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from feature_extractor import tfidf_transformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Function for getting the tfidf-based feature vectors, using Bag of Words feature vectors obtained from above function.\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm = 'l2', smooth_idf=True, use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "#Building tfidf transformer.\n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)\n",
    "\n",
    "#Showing tfidf features for test dataset using built tfidf transformer.\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Building fuction for extracting Tf-Idf based features.\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    #Using L2 norm.\n",
    "    vectorizer = TfidfVectorizer(min_df = 1, norm = 'l2', smooth_idf = True, use_idf = True, ngram_range = ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2am  alone  already  baby  become  best  boss  both  bully   buy  ...  \\\n",
      "0  0.00   0.00     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "1  0.00   0.00     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "2  0.00   0.00     0.00  0.00    0.00  0.00  0.71   0.0   0.71  0.00  ...   \n",
      "3  0.00   0.58     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "4  0.00   0.00     0.41  0.00    0.00  0.00  0.00   0.0   0.00  0.41  ...   \n",
      "5  0.00   0.00     0.00  0.00    0.00  0.41  0.00   0.0   0.00  0.00  ...   \n",
      "6  0.41   0.00     0.00  0.41    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "7  0.00   0.00     0.00  0.00    0.00  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "8  0.00   0.00     0.00  0.00    0.00  0.00  0.00   1.0   0.00  0.00  ...   \n",
      "9  0.00   0.00     0.00  0.00    0.41  0.00  0.00   0.0   0.00  0.00  ...   \n",
      "\n",
      "   respond   sad   san  shameless  smile  sons  sooo  soooo  would   wow  \n",
      "0     0.58  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.58  0.00  \n",
      "1     0.00  0.45  0.45       0.00   0.00  0.00  0.45   0.00   0.00  0.00  \n",
      "2     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "3     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "4     0.00  0.00  0.00       0.00   0.00  0.41  0.00   0.00   0.00  0.00  \n",
      "5     0.00  0.00  0.00       0.41   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "6     0.00  0.00  0.00       0.00   0.41  0.00  0.00   0.00   0.00  0.00  \n",
      "7     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.71   0.00  0.00  \n",
      "8     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.00  \n",
      "9     0.00  0.00  0.00       0.00   0.00  0.00  0.00   0.00   0.00  0.41  \n",
      "\n",
      "[10 rows x 40 columns]\n",
      "   2am  alone  already  baby  become  best  boss  both  bully  buy  ...  \\\n",
      "0  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "1  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "2  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "3  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "4  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "5  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "6  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "7  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "8  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "9  0.0    0.0      0.0   0.0     0.0   0.0   0.0   0.0    0.0  0.0  ...   \n",
      "\n",
      "   respond  sad  san  shameless  smile  sons  sooo  soooo  would  wow  \n",
      "0      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "1      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "2      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "3      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "4      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "5      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "6      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "7      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "8      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "9      0.0  0.0  0.0        0.0    0.0   0.0   0.0    0.0    0.0  0.0  \n",
      "\n",
      "[10 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "#Building tfidf vectorizer and getting training corpus feature vectors.\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)\n",
    "\n",
    "#Getting tfidf feature vector for the test dataset.\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'would', 'respond', 'i', 'go'], ['sooo', 'sad', 'i', 'miss', 'san', 'diego'], ['boss', 'bully'], ['interview', 'leave', 'alone'], ['sons', 'could', 'put', 'release', 'already', 'buy'], ['shameless', 'plug', 'best', 'rangers', 'forum', 'earth'], ['2am', 'feed', 'baby', 'fun', 'smile', 'coo'], ['soooo', 'high'], ['both'], ['journey', 'wow', 'u', 'become', 'cooler', 'hehe', 'possible']]\n",
      "[-0.0120806  -0.04740473  0.00549438  0.02106401 -0.01556838  0.04457872\n",
      " -0.01424363 -0.00197872 -0.04770748 -0.01278542]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhair\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "#Tokenizing corpora.\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "#TOKENIZED_CORPUS = CORPUS\n",
    "#tokenized_new_doc = new_doc\n",
    "\n",
    "print(TOKENIZED_CORPUS)\n",
    "#Building the word2vec model on our training corpus\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size = 10, window = 10, min_count = 1, sample = 1e-3)\n",
    "\n",
    "#Printing the vector for an example.\n",
    "print(model['would'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute tfidf weighted averaged word vector for a document\n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
    "                   if tfidf_vocabulary.get(word) \n",
    "                   else 0 \n",
    "                   for word in words]\n",
    "    \n",
    "    word_tfidf_map = {word:tfidf_val \n",
    "                      for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    feature_vector = np.zeros((num_features,), dtype = \"float64\")\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# generalize above function for a corpus of documents\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, tfidf_vocabulary, model, num_features):\n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf in zip(corpus, tfidf_vectors)]\n",
    "    \n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary, model, num_features)\n",
    "                for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.008 -0.023 -0.01  -0.008  0.002  0.006 -0.022  0.013 -0.029  0.013]\n",
      " [ 0.005 -0.014  0.003 -0.016  0.003 -0.015  0.007  0.009 -0.007  0.012]\n",
      " [ 0.009  0.015 -0.028  0.012 -0.007 -0.005 -0.014  0.043 -0.028  0.01 ]\n",
      " [ 0.013  0.013  0.022 -0.014  0.023 -0.018  0.017  0.003  0.006  0.015]\n",
      " [-0.005 -0.     0.018 -0.008 -0.012  0.009  0.015  0.014  0.011  0.019]\n",
      " [ 0.006 -0.019  0.006 -0.007 -0.    -0.006 -0.007  0.007 -0.007 -0.008]\n",
      " [-0.023  0.007  0.023 -0.    -0.004  0.007 -0.01   0.029  0.011  0.011]\n",
      " [-0.006  0.002 -0.028 -0.002 -0.022 -0.004  0.027  0.026 -0.013  0.003]\n",
      " [ 0.015 -0.022 -0.014  0.021 -0.016  0.034 -0.041 -0.005  0.007 -0.038]\n",
      " [ 0.014  0.009 -0.003 -0.016  0.01  -0.    -0.017 -0.004 -0.003 -0.02 ]]\n",
      "[[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.031  0.007  0.045 -0.011  0.037  0.033 -0.037  0.037  0.029  0.02 ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.001  0.031  0.03  -0.044  0.032  0.008  0.045  0.041  0.042 -0.004]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhair\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# get tfidf weights and vocabulary from earlier results and compute result\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus = TOKENIZED_CORPUS, \n",
    "                                                                     tfidf_vectors = corpus_tfidf,\n",
    "tfidf_vocabulary = vocab, model = model, num_features = 10)\n",
    "print(np.round(wt_tfidf_word_vec_features, 3))\n",
    "\n",
    "# compute avgd word vector for test new_doc\n",
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus = tokenized_new_doc, \n",
    "                                                                        tfidf_vectors = nd_tfidf, \n",
    "                                                                        tfidf_vocabulary = vocab, model = model, \n",
    "                                                                        num_features = 10)\n",
    "print(np.round(nd_wt_tfidf_word_vec_features, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-593528365eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds\n",
      "of\n",
      "people\n",
      "have\n",
      "been\n",
      "forced\n",
      "to\n",
      "their\n",
      "homes\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-876ddc93028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCORPUS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec(sentences = CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'would', 'respond', 'i', 'go'],\n",
       " ['sooo', 'sad', 'i', 'miss', 'san', 'diego'],\n",
       " ['boss', 'bully'],\n",
       " ['interview', 'leave', 'alone'],\n",
       " ['sons', 'could', 'put', 'release', 'already', 'buy'],\n",
       " ['shameless', 'plug', 'best', 'rangers', 'forum', 'earth'],\n",
       " ['2am', 'feed', 'baby', 'fun', 'smile', 'coo'],\n",
       " ['soooo', 'high'],\n",
       " ['both'],\n",
       " ['journey', 'wow', 'u', 'become', 'cooler', 'hehe', 'possible']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_cameroon = model.wv['smile']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'would', 'respond', 'i', 'go'], ['sooo', 'sad', 'i', 'miss', 'san', 'diego'], ['boss', 'bully'], ['interview', 'leave', 'alone'], ['sons', 'could', 'put', 'release', 'already', 'buy'], ['shameless', 'plug', 'best', 'rangers', 'forum', 'earth'], ['2am', 'feed', 'baby', 'fun', 'smile', 'coo'], ['soooo', 'high'], ['both'], ['journey', 'wow', 'u', 'become', 'cooler', 'hehe', 'possible']]\n",
      "[ 0.04858509 -0.03346521 -0.03632755 -0.00503827 -0.02940086 -0.00320018\n",
      "  0.02309685 -0.02500693 -0.0399977   0.03995411]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhair\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "#Tokenizing corpora.\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "#TOKENIZED_CORPUS = CORPUS\n",
    "#tokenized_new_doc = new_doc\n",
    "\n",
    "print(TOKENIZED_CORPUS)\n",
    "#Building the word2vec model on our training corpus\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size = 10, window = 10, min_count = 1, sample = 1e-3)\n",
    "\n",
    "#Printing the vector for an example.\n",
    "print(model['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keyedvectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_normalization(train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i would respond i go',\n",
       " 'sooo sad i miss san diego',\n",
       " 'boss bully',\n",
       " 'interview leave alone',\n",
       " 'sons could put release already buy',\n",
       " 'shameless plug best rangers forum earth',\n",
       " '2am feed baby fun smile coo',\n",
       " 'soooo high',\n",
       " 'both',\n",
       " 'journey wow u become cooler hehe possible']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "    feature_type = feature_type.lower().strip()\n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    \n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    \n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "    \n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    return (vectorizer, feature_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=0.0, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " <10x40 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 40 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_feature_matrix(data, feature_type = 'tfidf', ngram_range = (1, 1), min_df = 0.0, max_df = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
